<html class="gr__richzhang_github_io"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./The Unreasonable Effectiveness of Deep Networks as a Perceptual Metric_files/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	h1 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

  .responsive-video {
    width: 100%;
    height: auto;
  }

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style> 
		<title>SCOTCH and SODA: A Transformer Video Shadow Detection Framework</title>
  </head>

  <body data-gr-c-s-loaded="true">
    <br>
          <center>
          	<span style="font-size:32px">SCOTCH and SODA: A Transformer Video Shadow Detection Framework</span><br>
	  		  
	  		  <br>

	  		  <table align="center" width="900px">
	  			  <tbody><tr>
	  	              <td align="center" width="150px">
	  					<center>
	  						<span style="font-size:22px">Lihao Liu<sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="150px">
	  					<center>
	  						<span style="font-size:22px">Jean Prost<sup>2</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="150px">
	  					<center>
	  						<span style="font-size:22px">Lei Zhu<sup>3,4</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="200px">
	  					<center>
	  						<span style="font-size:22px">Nicolas Papadakis<sup>2</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="150px">
	  	        <center>
	  						<span style="font-size:22px">Pietro Liò<sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  			  </tr></tbody>
			  	</table>
			  	<table align="center" width="600px">
	  			  <tbody><tr>
	  			  				<td align="center" width="150px">
	  			  	<center>
	  						<span style="font-size:22px">Carola-Bibiane Schönlieb<sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="150px">
	  					<center>
	  						<span style="font-size:22px">Angelica I Aviles-Rivero<sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  			  </tr></tbody>
			  	</table>

					<br>

			  	<table align="center" width="850px">
	  			  <tbody><tr>
	  			  				<td align="center" width="140px">
	  			  	<center>
	  						<span style="font-size:18px"><sup>1</sup>University of Cambridge</span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="120px">
	  	        <center>
	  						<span style="font-size:18px">&nbsp&nbsp&nbsp&nbsp&nbsp<sup>2</sup>Universite de Bordeaux</span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="120px">
	  					<center>
	  						<span style="font-size:18px"><sup>3</sup>HKUST (GZ)</span>
		  		  		</center>
		  		  	  </td>
		  		  	  		<td align="center" width="40px">
	  					<center>
	  						<span style="font-size:18px"><sup>4</sup>HKUST</span>
		  		  		</center>
		  		  	  </td>
	  			  </tr></tbody>
			  	</table>

          <br>

	  		  <table align="center" width="900px">
	  			  <tbody><tr>
	  	              <td width="50px">
	  					<center>
	  	              <img class="rounded" src="./figures/segmentation_results.png" width="850px">
							</center>
		  	            </td>
		  	           </tr>
		  	    </tbody>
	  	    </table>

	  		  <br><br><hr>

  		  	<center><h1>Abstract</h1></center>
	  		  	<table align="center" width="900px">
	  					<tbody>
								<tr><td width="900px"><left>
		  						Shadows in videos are difficult to detect because of the large shadow deformation between frames. In this work, we argue that accounting for the shadow deformation is essential when designing a video shadow detection method. To this end, we introduce the shadow deformation attention trajectory (SODA), a new type of video self-attention module, specially designed to handle the large shadow deformations in videos. Moreover, we present a shadow contrastive learning mechanism (SCOTCH), which aims at guiding the network to learn a high-level representation of shadows, unified across different videos. We demonstrate empirically the effectiveness of our two contributions in an ablation study. Furthermore, we show that SCOTCH and SODA significantly outperforms existing techniques for video shadow detection.
								</left></td></tr>
							</tbody>
						</table>

					<br><br><hr>

					<center><h1>Overview of Network Architecture</h1></center>
			  		<table align="center" width="500px">
			  			<tbody>
			  			  	<tr>
			  			  		<td align="center"><img class="round" style="width:500px" src="./figures/network.png"></td>	
						  		</tr>
							</tbody>
						</table>

					<br><br><hr>

					<center><h1>Deformation Attention Trajectory</h1></center>
			  		<table align="center" width="900px">
			  			<tbody>
			  			  	<tr>
			  			  		<td align="center"><img class="round" style="width:900px" src="./figures/deformation_trajectory_attention.png"></td>	
						  		</tr>
							</tbody>
						</table>

					<br><br><hr>

					<center><h1>Video Shadow Detection Results</h1></center>
						<p align="center">
							<!-- <iframe width="960" height="540" src="./videos/video_shadow_detection_results.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center" alt="Scotch and Soda">
							</iframe> -->
							<video class="responsive-video" controls controlsList="nodownload" preload="metadata">
  						<source src="./videos/video_shadow_detection_results.mp4" type="video/mp4">
  						Your browser does not support the video tag.
							</video>
						</p>
						
					<br><br><hr>

	  		  <center><h1>Paper and Code</h1></center>
	  		  <table align="center" width="650px">
	  			  <tbody><tr>
					  <td><a href=""><img class="layered-paper-big" style="height:175px" src="./figures/paper.png"></a></td>
					  <td><span style="font-size:12pt">Lihao Liu, Jean Prost, Lei Zhu, Nicolas Papadakis, Pietro Liò, <br>Carola-Bibiane Schönlieb, and Angelica I Aviles-Rivero.</span><br><br>
					  <b><span style="font-size:12pt">SCOTCH and SODA: A Transformer Video Shadow Detection Framework.</span></b><br><br>
					  <span style="font-size:12pt">Computer Vision and Pattern Recognition (<b><font color="#FF0000">CVPR</font></b>), 2023.<br><br>
					  <a href="https://arxiv.org/abs/2211.06885" target="_blank">[arxiv]</a> 
					  <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:LMpZIn_iN7UJ:scholar.google.com/&output=citation&scisdr=CgUCEWi3ENfhkiZBiag:AAGBfm0AAAAAY7xHkahg-KlMznumBhg76NsUxStYwXU4&scisig=AAGBfm0AAAAAY7xHkf3zHk45paZANDGkBKM3WfJrL3os&scisf=4&ct=citation&cd=-1&hl=en" target="_blank">[bibtex]</a> 
					  <a href="https://github.com/lihaoliu-cambridge/scotch_and_soda" target="_blank">[code]</a> 
					  </td>
	  	      </tr></tbody>
	  	    </table>

			  	<br><br><hr>

	  			<center><h1>Acknowledgments</h1></center>
  		  	<table align="center" width="900px">
  					<tbody>
							<tr><td width="900px"><left>
	  						LL gratefully acknowledges the financial support from a GSK scholarship and a Girton College Graduate Research Fellowship at the University of Cambridge. AIAR acknowledges support from CMIH and CCIMI, University of Cambridge. CBS acknowledges support from the Philip Leverhulme Prize, the Royal Society Wolfson Fellowship, the EPSRC advanced career fellowship EP/V029428/1, EPSRC grants EP/S026045/1, EP/T003553/1, EP/N014588/1, EP/T017961/1, the Wellcome Innovator Awards 215733/Z/19/Z and 221633/Z/20/Z, the European Union Horizon 2020 research and innovation programme under the Marie Skodowska-Curie grant agreement No. 777826 NoMADS, the Cantab Capital Institute for the Mathematics of Information and the Alan Turing Institute. JP and NP acknowledge the supports from the EU Horizon 2020 research and innovation programme NoMADS (Marie Skłodowska-Curie grant agreement No 777826).
							</left></td></tr>
						</tbody>
					</table>
		<br><br>

<!-- Global site tag (gtag.js) - Google Analytics -->



</body></html>
