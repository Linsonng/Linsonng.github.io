<!DOCTYPE HTML>
<html>
	<head>
		<title>Pengze Li</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<style>
        p {
            text-align: justify;
        }
    </style>

	</head>
	<body class="is-preload">

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><img src="images/pic.png" alt="" /></span>
					<h1 id="logo"><a href="#">Pengze Li</a></h1>
					<p>Welcome to my page!</p>
				</header>
				<nav id="nav">
					<ul>
						<li><a href="#one" class="active">About</a></li>
						<li><a href="#two">Things I Can Do</a></li>
						<li><a href="#Two_point_five">Educations</a></li>
						<li><a href="#three">Work & Research Experience</a></li>
						<li><a href="#four">Others</a></li>
						<ul class="icons"><li><a href="https://github.com/Linsonng" class="icon brands fa-github"><span class="label">Github</span></a></li>
					</ul>
					
				</nav>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">



						<!-- One -->
							<section id="one">
								<div class="image main" data-position="center">
									<img src="images/banner.png" alt="" />
									
								</div>
								<div class="container">
									<p>* Except for my face, this oil painting was created by AI.</p>
									<header class="major">
										<h2>Hi there! I am Pengze Li</h2>
										<!-- template freebie by <a href="http://html5up.net">HTML5 UP</a>.-->
										<p>E-mail:  	LINSONNG@163.COM<br /></p>
									</header>
									<p>I am a college student majoring in Computer Graphics, Vision, and Imaging at University College London (UCL). My primary research interest lies in the intersection of machine learning with visual domains, encompassing areas like Artificial Intelligence Generated Content (AIGC), image processing, and virtual reality.</p>

									<p>In the realm of AIGC, I am deeply interested in developing 'text-to-visual' models rooted in latent diffusion techniques. The very ultimate vision is to create models capable of producing logical and imaginative films straight from scripts.</p>

									<p>As for virtual reality (VR), I am bullish about its potential. I believe in its transformative capacity as a display medium and am eager to drive research that propels VR to commonplace use.</p>



								</div>
							</section>

						<!-- Two -->
							<section id="two">
								<div class="container">
									<h3>Key Skills</h3>
									
									<ul class="feature-icons">
										<li class="icon solid fa-book">Python (>3 years)</li>
										<li class="icon solid fa-bolt">Deep Learning (Pytorch/TensorFlow/...)</li>
										<li class="icon solid fa-users">AIGC (Diffusion model, Text2Image)</li>							
										
										<li class="icon solid fa-cubes">Unity (Virtual Reality, C#)</li>
										<li class="icon solid fa-code">Matlab (Modeling, Data Processing)</li>
										
										
										<li class="icon solid fa-coffee">Drink coffee</li>
										
									</ul>
								</div>
							</section>

						<!-- Two_point_five -->
							<section id="Two_point_five">
								<div class="container">
									<h3>Educations</h3>
									
									<ul class="feature-icons">
									<section>
										<!-- <h4>Table</h4> -->
										
										<div class="table-wrapper">
											<table>
												<thead>
													<tr>
														<th>Institution</th>
														<th>Degree (GPA)</th>
														<th>Year</th>
													</tr>
												</thead>
												<tbody>
													<tr>
														<td>University College London (UCL)</td>
														<td>MSc. in Computer Graphics, Vision and Imaging (Distinction)</td>
														<td>2022-now</td>
													</tr>
													<tr>
														<td>Beijing University of Posts and Telecommunications (BUPT)</td>
														<td>B.Eng. in Electronic Engineering (First-Class)</td>
														<td>2018-2022</td>
													</tr>
													<tr>
														<td>Beijing 101 Middle School</td>
														<td>High School Diploma (A)</td>
														<td>2015-2018</td>
													</tr>
												</tbody>

											</table>
										</div>

									</section>
										
									</ul>
								</div>
							</section>

						<!-- Three -->
							<section id="three">
								<div class="container">
									<h3>Work Experience  &  Research Experience</h3>
									<!--<p>Integer eu ante ornare amet commetus vestibulum blandit integer in curae ac faucibus integer non. Adipiscing cubilia elementum integer. Integer eu ante ornare amet commetus.</p>-->
									<div class="features">

										<article>
				
											<p><span class="image right"><img src="images/text2hologram.png" alt="" />							 
											</span></p>		

												<h4>Text2hologram: describe your hologram here</h4>
												<h5>04/2023 - 09/2023</h5>	
												<h5>Academic Supervisor: Dr. Kaan Aksit</h5>
												<p>The fusion of natural language processing, image processing, and holography paves the way for unprecedented innovations in the field of computer-generated holography. This dissertation presents text2hologram, a groundbreaking pipeline that leverages deep learning models to transform textual descriptions into phase-only holograms. Aimed at democratizing access to holography and stimulating further research, the pipeline features a modular architecture and offers diverse interaction modes to cater to a wide range of users. This research employs a comprehensive methodology, that includes text-to-image conversion, super-resolution, depth estimation, hologram generation, and reconstruction at specific depth levels. The pipeline's user-centered design is emphasized, highlighting its focus on ease-of-use and reproducibility. Results and sample outcomes are presented, demonstrating the pipeline's efficacy through various intermediate outputs. The dissertation also concludes with a critical discussion where the novelty of the pipeline, its limitations, and potential future directions are examined. The source code is released to both <b><a target="_blank" href="https://pypi.org/project/text2hologram/">PyPI</a></b> and <b><a target="_blank" href="https://github.com/Linsonng/text2hologram/tree/main">Github</a></b> platforms, and an online Colab web tutorial is presented at <b><a target="_blank" href="https://colab.research.google.com/github/Linsonng/text2hologram/blob/main/text2hologram.ipynb">here</a></b></p>	
											
										</article>

										<article>
											<a href="#" class="image right"><img src="images/Seg.jpg" alt="" /></a>

											
												<h4>Multi-Task Learning for Segmentation of Male Pelvic Structures in MRI</h4>
												<h5>03/2023 - 05/2023</h5>
												<h5>Academic Supervisor: Dr Andre Altmann</h5>	
												<p>This research utilizes<b> <a target="_blank" href="https://zenodo.org/record/7013610#.ZGmSj3bMI7c">a publicly accessible database from UCL</a></b>, comprising 589 T2-weighted MR images of the male pelvic region, with eight anatomical structures segmented. Our approach incorporates<b> multi-task learning (MTL)</b> segmentation for precise definition of these crucial areas. This method enhances the segmentation accuracy by drawing information from adjacent structures with distinct shape characteristics. Tested on four anatomical regions with blurred boundaries and variable shapes, our MTL approach demonstrated superior accuracy, outperforming standalone tasks, the baseline U-Net, and other multitask models. The report is at <b><a target="_blank" href="https://github.com/Linsonng/Linsonng.github.io/blob/main/images/MTLreport.pdf">here</a></b>.</p>	
											
										</article>

										<article>
											<a href="#" class="image right">
												<video width="340" height="200" controls>
												    <source src="images/GamePlay.mp4" type="video/mp4">								 
												</video></a>

											
												<h4>3-D Painting Virtual Reality Platform</h4>
												<h5>02/2023 - 04/2023</h5>	
												<h5>Academic Supervisor: Prof. Anthony Steed</h5>
												<p>In cooperation with Lucien Li, I completed a <b>virtual reality</b> platform that provides the function of <b>drawing in 3D space</b> and supports <b>online multiplayer collaboration</b>. The platform is done using <b>Unity</b>, run in Unity or a VR headset. The platform is based on Ubiq, UCL's open-source social virtual reality platform. On the left is the perspective of the player in the virtual environment, and another player is drawing eyes on the snowman. The complete final report is at <b><a target="_blank" href="https://github.com/Linsonng/UCL-VR-2023/blob/main/Docs/report.pdf">here</a></b>. </p>	
											
										</article>

										<article>
											<a href="#" class="image right"><img src="images/MSF.jpg" alt="" /></a>

											
												<h4>Mean Curvature Skeletons</h4>
												<h5>04/2023 - 05/2023</h5>	
												<h5>Academic Supervisor: Prof. Niloy J. Mitra</h5>
												<p>Building on the concepts in "Mean Curvature Skeletons," this study creates an open-source <b>Python</b> model for deriving skeletons from <b>3D objects</b>. Using Voronoi poles and the Laplacian matrix, our model iteratively solves equations to approximate an object's skeleton, identifying points near this structure. The model's efficiency is enhanced through optimizations in collapse, eigenvector reconstruction, and Laplace smoothing. This method effectively determines the skeletal structure of complex objects, marking the first open-source Python implementation of this kind. The open source code is at <b><a target="_blank" href="https://github.com/Linsonng/MSF">here</a></b>.</p>	
											
										</article>

										<article>
											<a href="#" class="image right"><img src="images/HumanData.jpg" alt="" /></a>
											
												<h4>Pedestrian Classification</h4>
												<h5>01/2022 - 06/2022</h5>	
												<h5>Academic Supervisor: Dr. Da Guo, Prof. Xile Cao</h5>
												<p>My graduation project was cooperated with the engineers from <a target="_blank" href="https://www.aidong-ai.com/">Aidong Beyond Artificial Intelligence Technology (Beijing) Co., Ltd.</a> Taking advantage of the <b>private dataset of pedestrians</b> in the factory provided by Aidong, I processed real industrial datasets and completed a pedestrian classification model based on <b>ResNet</b>, in which <b>data augmentation</b> and <b>SE Module</b> are applied. The accuracy (TP+TN)/(TP+FN+FP+TN) is greater than 96% . Considering the needs of the industry, this project ran with <b>TensorFlow</b>.</p>	
											
										</article>
										
										<article>
										
											<p><span class="image right"><a href="#" class="image">
												<video width="340" height="200" controls>
												    <source src="images/snowboard1.mp4" type="video/mp4">								 
												</video></a>
											<h4>Image Processing Intern</h4>
											<h5>06/2021 - 06/2022</h5>
											<h5>Institute of Automation,<br> Chinese Academy of Science</h5>


											<h5>Academic Supervisor: <br><a target="_blank" href="https://scholar.google.com/citations?user=o8PT69EAAAAJ&hl=zh-CN">Prof. Jian Cheng</a>, Dr. Chenghua Li</h5>
												
												<p>Job Responsibilities:<br>
													* Stay updated with the state-of-the-art <b>computer vision</b> and <b>deep learning</b> algorithms;<br>
													* Implement <b>convolutional neural network (CNN) models</b> on GPU, based on <b>Python</b> and <b>Pytorch</b>;<br>
													* Focus on <b>real-time denoising</b> algorithms, with an emphasis on <b>U-Net-based</b> approaches.<br>
													* Design, train, evaluate, tune, and accelerate models, one of the results is shown below.<br>
												The video shows the denoising results (25 frames) given by a convolutional neural network in 0.5 seconds.</p>
										</article>

										<article>
											<a href="#" class="image right"><video width="300" height="300" controls>
											    <source src="images/RoadExtraction.mp4" type="video/mp4">
											    Your browser does not support the video tag. Maybe try to switch to Google Chrome.
											</video></a>
											
											

												<h4>Aerial Image Road Extraction</h4>
												<h5>09/2020 - 06/2022</h5>	
												<h5>Academic Supervisor: Dr. Junli Yang</h5>
												<p>In the <b>CVPR DeepGlobe Road Extraction Challenge</b>, our team developed a novel approach called Multiscale Strip Pooling DLinkNet. We designed a fusion model incorporating various architectures, including <b>ResNet, D-LinkNet, SENet, D-block, Transformer</b>, and several other modules. By employing a pooling feature map to address the challenges associated with narrow and slender features in road extraction tasks, we achieved notable improvements in IoU scores. A patent for this work is currently pending (Patent Application No.: 2023106206149), and a paper detailing our findings will be submitted to the journal Remote Sensing.</p>	
											
										</article>

										
										<article>
											<a href="#" class="image right"><img src="images/NUS.jpg" alt="" /></a>

											
												<h4>National University of Singapore Online Winter Programme: Artificial Intelligence and Machine Learning</h4>
												<h5>01/2021 - 02/2021</h5>	
												<h5>Academic Supervisor: Prof. Mehul Motani</h5>
												<p>Supervised by Assoc. Prof. Mehul Motani, I acquired foundational knowledge in supervised learning algorithms such as Linear Regression, Decision Tree, SVM, and Neural Networks. The program offered hands-on experience in utilizing Python and Jupyter Notebook, incorporating libraries such as Pandas, NumPy, and Scikit-learn. My key project during this period involved predicting the future populations of Singapore and China using the learned models, executed in a team-oriented environment. </p>	
											
										</article>

									</div>
								</div>
							</section>

						<!-- Four -->
							<section id="four">
								<div class="container">
									<h3>Others</h3>
									<p>College Students’ Innovative Entrepreneurial Training Plan Programs</p>
									<div class="table-wrapper">
											<table>
												<thead>
													<tr>
														<th>Title</th>
														<th>Supervisor</th>
														<th>Year</th>
													</tr>
												</thead>
												<tbody>
													<tr>
														<td>The Proper Noun Translator in Thai-and-Chinese Language Based on Python</td>
														<td>Prof. Jianming Huang</td>
														<td>06/2020-05/2021</td>
													</tr>
													<tr>
														<td>Internet Meme Searching & Compositing Tool Based on Crawling and Artificial</td>
														<td>Prof. Yang Ji</td>
														<td>09/2019-07/2020</td>
													</tr>
													<tr>
														<td>College Campus Outdoor-scene Navigation System Based on Unity 3D</td>
														<td>Yuan Sun</td>
														<td>06/2019-07/2020</td>
													</tr>
												</tbody>
											</table>
										</div>

										<p>HONORS & AWARDS</p>
										<p>School-level Scholarship, BUPT, Oct. 2021<br>
										School-level Scholarship, BUPT, Oct. 2020</p>

						

					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>&copy; Pengze Li</li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
